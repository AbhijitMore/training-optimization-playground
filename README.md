# ğŸ§‘â€ğŸ’» Training Optimization Playground ğŸ¯

Welcome to **Training Optimization Playground**! This repository is a fun and interactive space where we explore various **training optimizations** for neural networks. Whether youâ€™re curious about how different optimizers perform or how to find the optimal learning rate for your models, this playground has something for you!

## ğŸš€ Project Overview
In this repository, we cover:
- ğŸ“Š **Optimizer Comparison**: Compare different optimizers like `SGD`, `Adam`, and `RMSprop`.
- ğŸ” **Learning Rate Finder**: Automatically find the best learning rate to speed up training without sacrificing accuracy.
- Detailed performance metrics are plotted for easy comparison of experiments.

This repository is divided into two parts:
1. **Optimizer Comparison** (`optimizer_comparison/`): Experimenting with multiple optimizers and analyzing their performance.
2. **Learning Rate Finder** (`find_optimal_lr/`): Finding the optimal learning rate for better training.

---

## ğŸ“‚ Directory Structure

```
training-optimization-playground/
â”œâ”€â”€ optimizer_comparison/
â”‚   â”œâ”€â”€ resources/
â”‚   â”‚   â”œâ”€â”€ console_output.png  # Sample console output from optimizer comparison
â”‚   â”‚   â”œâ”€â”€ metrics.png         # Metrics comparison between different learning rates
â”‚   â”œâ”€â”€ utils.py                # Utility functions for loading data, plotting metrics
â”‚   â”œâ”€â”€ nets.py                 # Simple neural network model definitions
â”‚   â”œâ”€â”€ main.py                 # Main script to run optimizer comparison experiments
â”œâ”€â”€ find_optimal_lr/
â”‚   â”œâ”€â”€ resources/
â”‚   â”‚   â”œâ”€â”€ learning_rate.png    # LR Finder graph
â”‚   â”‚   â”œâ”€â”€ console_output.png   # Sample console output from LR finding process
â”‚   â”‚   â”œâ”€â”€ metrics_comparison.png # Metrics comparison after applying optimal learning rate
â”‚   â”œâ”€â”€ find_lr.py               # Script for finding optimal learning rate
â”‚   â”œâ”€â”€ main.py                  # Main script to demonstrate training with optimal LR
```

---

## ğŸ› ï¸ Usage

### Optimizer Comparison

Run the **optimizer comparison** experiment to see how `SGD`, `Adam`, and `RMSprop` perform with your dataset.

```bash
cd optimizer_comparison/
python main.py
```

The **console output** will be saved to the `resources/` directory.

![Optimizer Console Output](optimizer_comparison/resources/console_output.png)

---

### Learning Rate Finder

Run the **learning rate finder** to determine the optimal learning rate for your model.

```bash
cd find_optimal_lr/
python main.py
```

The **learning rate plot** and **console output** will be saved to the `resources/` directory.

![Learning Rate Plot](find_optimal_lr/resources/learning_rate.png)
![Learning Rate Plot](find_optimal_lr/resources/metrics_comparison.png)



---

## ğŸ“ˆ Results and Visualization

- **Optimizer Comparison**: We experiment with different optimizers and visualize their performance. Check out the plots in the `resources/` folder:
- ![Optimizer Comparison Metrics](optimizer_comparison/resources/metrics.png)

- **Learning Rate Finder**: Visualize the learning rates vs. loss and learning rates vs. accuaracy graph:
- ![Larning Rate Finder Metrics](find_optimal_lr/resources/metrics_comparison.png)

---

The accuracy and loss plots give a visual summary of model performance across epochs, indicating improvements and stability.

## Contributing
Contributions are welcome! ğŸ‰ Whether you're reporting a bug, suggesting a new feature, or submitting a pull request, your input is valuable.
